{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3017d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9533a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "bf = cv2.BFMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4be273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, percentage):\n",
    "    width = int(img.shape[1] *  percentage / 100)\n",
    "    height = int(img.shape[0] *  percentage / 100)\n",
    "    dim = (width, height)\n",
    "    \n",
    "    resized_img = cv2.resize(img, dim)\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2948ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract frames\n",
    "def extract_frames(video_path, output_folder):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    # Check if the video was successfully opened\n",
    "    if not video.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = video.read()\n",
    "        # If the frame was not successfully read, then we have reached the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "        # Construct the output file path for the current frame\n",
    "        output_path = os.path.join(output_folder, f\"frame_{frame_count}.png\")\n",
    "        # Save the frame as an image file\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        # Increment the frame count\n",
    "        frame_count += 1\n",
    "    # Release the video file\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94087e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template_1\n",
    "\n",
    "template_1 = cv2.imread('templates/frame_1.png', 0) #gray\n",
    "template_1 = resize_img(template_1, 50)\n",
    "keypoints_template_1, descriptors_template_1 = sift.detectAndCompute(template_1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbfa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template_11\n",
    "\n",
    "template_11 = cv2.imread('templates/frame_11.png', 0)\n",
    "template_11 = resize_img(template_11, 50)\n",
    "keypoints_template_11, descriptors_template_11 = sift.detectAndCompute(template_11, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03eb2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 20-30\n",
    "template_30 = cv2.imread('templates/frame_30.png', 0)\n",
    "template_30 = resize_img(template_30, 50)\n",
    "keypoints_template_30, descriptors_template_30 = sift.detectAndCompute(template_30, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3065f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 30-40\n",
    "\n",
    "template_35 = cv2.imread('templates/frame_35.png', 0)\n",
    "template_35 = resize_img(template_35, 50)\n",
    "keypoints_template_35, descriptors_template_35 = sift.detectAndCompute(template_35, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120de8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 40-50\n",
    "template_40 = cv2.imread('templates/frame_40.png', 0)\n",
    "template_40 = resize_img(template_40, 50)\n",
    "keypoints_template_40, descriptors_template_40 = sift.detectAndCompute(template_40, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248e4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 50-60\n",
    "template_50 = cv2.imread('templates/frame_50.png', 0)\n",
    "template_50 = resize_img(template_50, 50)\n",
    "keypoints_template_50, descriptors_template_50 = sift.detectAndCompute(template_50, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98cfbb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 60-70\n",
    "template_70 = cv2.imread('templates/frame_70.png', 0)\n",
    "template_70 = resize_img(template_70, 50)\n",
    "keypoints_template_70, descriptors_template_70 = sift.detectAndCompute(template_70, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7be376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 70-80\n",
    "template_80 = cv2.imread('templates/frame_80.png', 0)\n",
    "template_80 = resize_img(template_80, 50)\n",
    "keypoints_template_80, descriptors_template_80 = sift.detectAndCompute(template_80, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94969235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 80-90\n",
    "\n",
    "template_84 = cv2.imread('templates/frame_84.png', 0)#gray\n",
    "template_84 = resize_img(template_84, 50)\n",
    "keypoints_template_84, descriptors_template_84 = sift.detectAndCompute(template_84, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d6cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 90-100\n",
    "template_91 = cv2.imread('templates/frame_91.png', 0)#gray\n",
    "template_91 = resize_img(template_91, 50)\n",
    "keypoints_template_91, descriptors_template_91 = sift.detectAndCompute(template_91, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67e4adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 100-110\n",
    "template_101 = cv2.imread('templates/frame_101.png', 0)#gray\n",
    "template_101 = resize_img(template_101, 50)\n",
    "keypoints_template_101, descriptors_template_101 = sift.detectAndCompute(template_101, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c60495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 110-120\n",
    "template_110 = cv2.imread('templates/frame_110.png', 0)#gray\n",
    "template_110 = resize_img(template_110, 50)\n",
    "keypoints_template_110, descriptors_template_110 = sift.detectAndCompute(template_110, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "554788ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 120-130\n",
    "template_130 = cv2.imread('templates/frame_130.png', 0)#gray\n",
    "template_130 = resize_img(template_130, 50)\n",
    "keypoints_template_130, descriptors_template_130 = sift.detectAndCompute(template_130, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae743e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 130-140\n",
    "template_137 = cv2.imread('templates/frame_137.png', 0)#gray\n",
    "template_137 = resize_img(template_137, 50)\n",
    "keypoints_template_137, descriptors_template_137 = sift.detectAndCompute(template_137, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4501431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 140-150\n",
    "template_147 = cv2.imread('templates/frame_147.png', 0)#gray\n",
    "template_147 = resize_img(template_147, 50)\n",
    "keypoints_template_147, descriptors_template_147 = sift.detectAndCompute(template_147, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2365358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 150-160\n",
    "template_155 = cv2.imread('templates/frame_155.png', 0)#gray\n",
    "template_155 = resize_img(template_155, 50)\n",
    "keypoints_template_155, descriptors_template_155 = sift.detectAndCompute(template_155, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fcc62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 160-170\n",
    "\n",
    "template_163 = cv2.imread('templates/frame_163.png', 0)\n",
    "template_163 = resize_img(template_163, 50)\n",
    "keypoints_template_163, descriptors_template_163 = sift.detectAndCompute(template_163, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3955d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 170-180\n",
    "template_177 = cv2.imread('templates/frame_177.png', 0)\n",
    "template_177 = resize_img(template_177, 50)\n",
    "keypoints_template_177, descriptors_template_177 = sift.detectAndCompute(template_177, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ff99232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 180-190\n",
    "\n",
    "template_181 = cv2.imread('templates/frame_181.png', 0)\n",
    "template_181 = resize_img(template_181, 50)\n",
    "keypoints_template_181, descriptors_template_181 = sift.detectAndCompute(template_181, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db7f3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 190-200\n",
    "\n",
    "\n",
    "template_191 = cv2.imread('templates/frame_191.png', 0)\n",
    "template_191 = resize_img(template_191, 50)\n",
    "keypoints_template_191, descriptors_template_191 = sift.detectAndCompute(template_191, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a6a199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 200-210\n",
    "template_201 = cv2.imread('templates/frame_201.png', 0)\n",
    "template_201 = resize_img(template_201, 50)\n",
    "keypoints_template_201, descriptors_template_201 = sift.detectAndCompute(template_201, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "580ca277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 210-220\n",
    "template_211 = cv2.imread('templates/frame_211.png', 0)\n",
    "template_211 = resize_img(template_211, 50)\n",
    "keypoints_template_211, descriptors_template_211 = sift.detectAndCompute(template_211, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e34e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 220-230\n",
    "template_221 = cv2.imread('templates/frame_221.png', 0)\n",
    "template_221 = resize_img(template_221, 50)\n",
    "keypoints_template_221, descriptors_template_221 = sift.detectAndCompute(template_221, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e82e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 230-240\n",
    "template_233 = cv2.imread('templates/frame_233.png', 0)\n",
    "template_233 = resize_img(template_233, 50)\n",
    "keypoints_template_233, descriptors_template_233 = sift.detectAndCompute(template_233, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b8c6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 240-250\n",
    "template_241 = cv2.imread('templates/frame_241.png', 0)\n",
    "template_241 = resize_img(template_241, 50)\n",
    "keypoints_template_241, descriptors_template_241 = sift.detectAndCompute(template_241, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb0853a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 250-260\n",
    "template_251 = cv2.imread('templates/frame_251.png', 0)\n",
    "template_251 = resize_img(template_251, 50)\n",
    "keypoints_template_251, descriptors_template_251 = sift.detectAndCompute(template_251, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a261e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 260-270\n",
    "\n",
    "template_268 = cv2.imread('templates/frame_268.png', 0)\n",
    "template_268 = resize_img(template_268, 50)\n",
    "keypoints_template_268, descriptors_template_268 = sift.detectAndCompute(template_268, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca44e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 270-280\n",
    "template_271 = cv2.imread('templates/frame_271.png', 0)\n",
    "template_271 = resize_img(template_271, 50)\n",
    "keypoints_template_271, descriptors_template_271 = sift.detectAndCompute(template_271, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38ee2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 280-290\n",
    "template_281 = cv2.imread('templates/frame_281.png', 0)\n",
    "template_281 = resize_img(template_281, 50)\n",
    "keypoints_template_281, descriptors_template_281 = sift.detectAndCompute(template_281, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be6a8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 290-300\n",
    "template_291 = cv2.imread('templates/frame_291.png', 0)\n",
    "template_291 = resize_img(template_291, 50)\n",
    "keypoints_template_291, descriptors_template_291 = sift.detectAndCompute(template_291, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c029ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 300-310\n",
    "template_301 = cv2.imread('templates/frame_301.png', 0)\n",
    "template_301 = resize_img(template_301, 50)\n",
    "keypoints_template_301, descriptors_template_301 = sift.detectAndCompute(template_301, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2347ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 310-320\n",
    "template_311 = cv2.imread('templates/frame_311.png', 0)\n",
    "template_311 = resize_img(template_311, 50)\n",
    "keypoints_template_311, descriptors_template_311 = sift.detectAndCompute(template_311, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31801724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 320-330\n",
    "template_321 = cv2.imread('templates/frame_321.png', 0)\n",
    "template_321 = resize_img(template_321, 50)\n",
    "keypoints_template_321, descriptors_template_321 = sift.detectAndCompute(template_321, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "166d6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 330-340\n",
    "\n",
    "template_331 = cv2.imread('templates/frame_331.png', 0)\n",
    "template_331 = resize_img(template_331, 50)\n",
    "keypoints_template_331, descriptors_template_331 = sift.detectAndCompute(template_331, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00fd17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 340-350\n",
    "template_341 = cv2.imread('templates/frame_341.png', 0)\n",
    "template_341 = resize_img(template_341, 50)\n",
    "keypoints_template_341, descriptors_template_341 = sift.detectAndCompute(template_341, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91fca76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 350-360\n",
    "template_359 = cv2.imread('templates/frame_359.png', 0)\n",
    "template_359 = resize_img(template_359, 50)\n",
    "keypoints_template_359, descriptors_template_359 = sift.detectAndCompute(template_359, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0502da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 360-370\n",
    "template_361 = cv2.imread('templates/frame_361.png', 0)\n",
    "template_361 = resize_img(template_361, 50)\n",
    "keypoints_template_361, descriptors_template_361 = sift.detectAndCompute(template_361, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43a55c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 370-380\n",
    "template_371 = cv2.imread('templates/frame_371.png', 0)\n",
    "template_371 = resize_img(template_371, 50)\n",
    "keypoints_template_371, descriptors_template_371 = sift.detectAndCompute(template_371, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c3e3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 380-390\n",
    "template_386 = cv2.imread('templates/frame_386.png', 0)\n",
    "template_386 = resize_img(template_386, 50)\n",
    "keypoints_template_386, descriptors_template_386 = sift.detectAndCompute(template_386, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "675a0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 390-400\n",
    "template_391 = cv2.imread('templates/frame_391.png', 0)\n",
    "template_391 = resize_img(template_391, 50)\n",
    "keypoints_template_391, descriptors_template_391 = sift.detectAndCompute(template_391, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad4cf2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 400-410\n",
    "template_401 = cv2.imread('templates/frame_401.png', 0)\n",
    "template_401 = resize_img(template_401, 50)\n",
    "keypoints_template_401, descriptors_template_401 = sift.detectAndCompute(template_401, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01d35faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template 410-420\n",
    "template_411 = cv2.imread('templates/frame_411.png', 0)\n",
    "template_411 = resize_img(template_411, 50)\n",
    "keypoints_template_411, descriptors_template_411 = sift.detectAndCompute(template_411, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "847655e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frame Numbers that you select for templates!\n",
    "\n",
    "template_frames = [1, 11,30,35,40,50,70,80, 84,91,101,\n",
    "                   110,130,137,147,155, 163,177, 181, 191,201,\n",
    "                   211,221,233,241,251,268,271,281,291,\n",
    "                   301,311,321,331,341,359,361,371,386,391,\n",
    "                   401,411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a88027ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(r\"E:\\TA Filoger\\Exercise\\DIP\\DIP22\\media\\car.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7db328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "frame_number = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = resize_img(frame, 50)\n",
    "    keypoints_frame, descriptors_frame = sift.detectAndCompute(frame, None) \n",
    "    \n",
    "    \n",
    "    frame_number = frame_number + 1\n",
    "    \n",
    "    template_number = frame_number // 10 #int\n",
    "    print(template_number)\n",
    "    t ='template_{}'.format(template_frames[template_number-1])\n",
    "    f = 'keypoints_template_{}'.format(template_frames[template_number-1])\n",
    "    d = 'descriptors_template_{}'.format(template_frames[template_number-1])\n",
    "    \n",
    "    template = eval(t) #chabnge to val\n",
    "    keypoints_template = eval(f)\n",
    "    descriptors_template = eval(d)\n",
    "    \n",
    "    matches = bf.knnMatch(descriptors_template, descriptors_frame, k=2)\n",
    "    \n",
    "    good_matches = []\n",
    "    \n",
    "    if matches is not None:\n",
    "        \n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        \n",
    "        if len(good_matches) >= 4:\n",
    "\n",
    "            src_pts = np.float32([keypoints_template[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2) \n",
    "            dst_pts = np.float32([keypoints_frame[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            \n",
    "            #print('src_pts ' , src_pts)\n",
    "            #print('dst_pts' , dst_pts)\n",
    "\n",
    "            M,_=cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)\n",
    "            \n",
    "            h, w = template.shape\n",
    "         \n",
    "            if M is not None:\n",
    "                template_corner = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1, 1, 2)\n",
    "\n",
    "                transformed_corner = cv2.perspectiveTransform(template_corner, M)\n",
    "\n",
    "                bounding_box = cv2.polylines(frame, [np.int32(transformed_corner)], True, (0, 255, 0), 2)\n",
    "                cv2.imshow('bounding box', bounding_box)\n",
    "\n",
    "    \n",
    "    if cv2.waitKey(1)==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9b09d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img = cv2.imread(r\"E:\\TA Filoger\\Exercise\\DIP\\DIP22\\media\\Color.jpg\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77eaef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img = cv2.imread(r\"E:\\TA Filoger\\Exercise\\DIP\\DIP22\\media\\Color.jpg\")\n",
    "gray_img = cv2.cvtColor(color_img,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8dc8da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('gray',gray_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83ba9535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('gray',color_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d7fc512",
   "metadata": {},
   "source": [
    "[SURF](https://docs.opencv.org/3.4/df/dd2/tutorial_py_surf_intro.html)<br>\n",
    "[ORB](https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
